'use client';

import { AvatarCanvas } from "@/components/AvatarCanvas";
import { useEffect, useState, useRef } from "react";

// --- å‹å®šç¾© ---
interface ChatMessage {
  role: 'ai' | 'user';
  content: string;
}
interface SpeechRecognition extends EventTarget {
  lang: string;
  interimResults: boolean;
  continuous: boolean;
  onresult: (event: any) => void;
  onend: () => void;
  onerror?: (event: any) => void;
  start: () => void;
  stop: () => void;
}
declare global {
  interface Window {
    SpeechRecognition: { new(): SpeechRecognition };
    webkitSpeechRecognition: { new(): SpeechRecognition };
  }
}
// --- ã“ã“ã¾ã§ ---

export default function Home() {
  const [isTalking, setIsTalking] = useState(false);
  const [isLoading, setIsLoading] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [chatHistory, setChatHistory] = useState<ChatMessage[]>([]);
  const [currentTranscript, setCurrentTranscript] = useState<string>('');
  const [interviewStarted, setInterviewStarted] = useState(false);
  const [sessionId, setSessionId] = useState<string | null>(null);
  const [isFinished, setIsFinished] = useState(false);

  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const isInitializingRef = useRef(false);
  const isSubmittingRef = useRef(false);

  // è¿½åŠ ï¼šç¢ºå®šãƒ†ã‚­ã‚¹ãƒˆãƒ»æš«å®šãƒ†ã‚­ã‚¹ãƒˆã®ä¿æŒç”¨
  const finalTextRef = useRef<string>('');   // isFinal ã®åˆ†ã ã‘è“„ç©
  const interimTextRef = useRef<string>(''); // æš«å®šã¯æ¯å›ç½®ãæ›ãˆ

  // ã‚¢ãƒ³ãƒã‚¦ãƒ³ãƒˆæ™‚ã«éŒ²éŸ³ã‚’æ­¢ã‚ã‚‹
  useEffect(() => {
    return () => {
      try { recognitionRef.current?.stop(); } catch {}
    };
  }, []);

  const handleStartInterview = async () => {
    if (isInitializingRef.current) return;

    try {
      isInitializingRef.current = true;
      setInterviewStarted(true);
      setIsLoading(true);

      const response = await fetch('/api/interview', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ stage: "init" }),
      });
      if (!response.ok) throw new Error('API Error');

      const questionText = decodeURIComponent(response.headers.get('X-Question-Text') || "");
      const newSessionId = response.headers.get('X-Session-Id');

      setChatHistory([{ role: 'ai', content: questionText }]);
      if (newSessionId) setSessionId(newSessionId);

      const audioBlob = await response.blob();
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);
      audio.onended = () => setIsTalking(false);
      setIsTalking(true);
      audio.play();
    } catch (error) {
      console.error("Failed to start interview:", error);
      setChatHistory([{ role: 'ai', content: 'ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸã€‚' }]);
    } finally {
      setIsLoading(false);
      // é–‹å§‹ã¯ä¸€åº¦ãã‚Šæƒ³å®šã®ãŸã‚ãƒ•ãƒ©ã‚°è§£é™¤ã¯çœç•¥
    }
  };

  // --- éŒ²éŸ³é–‹å§‹ï¼ˆæš«å®šã¯ä¸Šæ›¸ããƒ»ç¢ºå®šã ã‘è“„ç©ï¼‰ ---
  const startRecording = () => {
    setIsRecording(true);
    // è¡¨ç¤ºã¨ãƒãƒƒãƒ•ã‚¡ã‚’åˆæœŸåŒ–
    setCurrentTranscript('');
    finalTextRef.current = '';
    interimTextRef.current = '';

    const SR = (window.SpeechRecognition || window.webkitSpeechRecognition);
    if (!SR) {
      alert("ãŠä½¿ã„ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°èªè­˜ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚");
      setIsRecording(false);
      return;
    }

    const recognition = new SR();
    recognition.lang = 'ja-JP';
    recognition.interimResults = true;
    recognition.continuous = true;
    recognitionRef.current = recognition;

    recognition.onresult = (event: any) => {
      let addedFinal = '';
      let newInterim = '';

      // éå»åˆ†ãŒå«ã¾ã‚Œã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§ resultIndex ã‹ã‚‰å‡¦ç†
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const res = event.results[i];
        const t: string = res[0].transcript;
        if (res.isFinal) {
          addedFinal += t;     // ç¢ºå®šã¯è“„ç©
        } else {
          newInterim = t;      // æš«å®šã¯ç½®ãæ›ãˆ
        }
      }

      if (addedFinal) finalTextRef.current += addedFinal;
      interimTextRef.current = newInterim;

      // ç”»é¢è¡¨ç¤ºã¯æ¯å›ã€Œä¸Šæ›¸ãã€
      const combined = (finalTextRef.current + interimTextRef.current)
        .replace(/\s+/g, ' ')
        .trim();
      setCurrentTranscript(combined);
    };

    recognition.onend = () => {
      setIsRecording(false);

      // æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆï¼ˆç¢ºå®šï¼‹æœ€å¾Œã®æš«å®šï¼‰ã‚’é€ä¿¡
      const finalToSend = (finalTextRef.current + interimTextRef.current).trim();
      // ãƒãƒƒãƒ•ã‚¡ã¨è¡¨ç¤ºã‚’ã‚¯ãƒªã‚¢
      finalTextRef.current = '';
      interimTextRef.current = '';
      setCurrentTranscript('');

      if (finalToSend) {
        sendToBackend(finalToSend);
      }
    };

    recognition.onerror = (e: any) => {
      console.error('SpeechRecognition error:', e);
    };

    recognition.start();
  };

  // --- éŒ²éŸ³åœæ­¢ ---
  const stopRecording = () => {
    recognitionRef.current?.stop();
  };

  // --- ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¸é€ä¿¡ ---
  const sendToBackend = async (message: string) => {
    if (isSubmittingRef.current) return;
    if (!sessionId) {
      alert("ã‚»ãƒƒã‚·ãƒ§ãƒ³IDãŒã‚ã‚Šã¾ã›ã‚“ã€‚");
      return;
    }

    try {
      isSubmittingRef.current = true;
      const userMessage: ChatMessage = { role: 'user', content: message };
      setChatHistory(prev => [...prev, userMessage]);
      setIsLoading(true);

      const response = await fetch('/api/interview', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          stage: "answer",
          sessionId: sessionId,
          answer: message,
          // ã‚µãƒ¼ãƒå´ã‚’æ‹¡å¼µã—ãŸã‚‰ final: true ã‚’ä»˜ã‘ã¦é€ã‚‹
          // final: true,
        }),
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        throw new Error(`API Error: ${JSON.stringify(errorData)}`);
      }

      const nextQuestionText = decodeURIComponent(response.headers.get('X-Question-Text') || "");
      const finished = response.headers.get('X-Finished') === 'true';
      setChatHistory(prev => [...prev, { role: 'ai', content: nextQuestionText }]);
      setIsFinished(finished);

      const audioBlob = await response.blob();
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);
      audio.onended = () => setIsTalking(false);
      setIsTalking(true);
      audio.play();
    } catch (error) {
      console.error(error);
      setChatHistory(prev => [...prev, { role: 'ai', content: 'ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚' }]);
    } finally {
      setIsLoading(false);
      isSubmittingRef.current = false;
    }
  };

  // æœ€æ–°ã®AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—ï¼ˆfindLastãŒãªã„ç’°å¢ƒã§ã‚‚å‹•ãã‚ˆã†ã«ï¼‰
  const latestAiQuestion = (() => {
    for (let i = chatHistory.length - 1; i >= 0; i--) {
      if (chatHistory[i].role === 'ai') return chatHistory[i].content;
    }
    return undefined;
  })();

  return (
    <main className="flex flex-row h-screen bg-gray-900 text-white font-sans">
      <div className="w-2/3 h-full relative">
        <AvatarCanvas isTalking={isTalking} />
      </div>

      <div className="w-1/3 h-full bg-slate-800 p-8 flex flex-col justify-between border-l-2 border-slate-600">
        <div>
          <h2 className="text-2xl font-bold text-teal-300 mb-4 border-b-2 border-teal-500 pb-2">
            AIé¢æ¥
          </h2>
          <div className="bg-slate-700 p-6 rounded-lg shadow-lg min-h-[120px]">
            <p className="text-lg text-gray-200 leading-relaxed">
              {!interviewStarted ? "ä¸‹ã®ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦é¢æ¥ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚" :
                isLoading ? "å¿œç­”ã‚’å¾…ã£ã¦ã„ã¾ã™..." :
                  isFinished ? "é¢æ¥ã¯çµ‚äº†ã§ã™ã€‚ãŠç–²ã‚Œæ§˜ã§ã—ãŸã€‚" :
                    latestAiQuestion || "..."}
            </p>
          </div>
        </div>

        {!interviewStarted ? (
          <div className="flex flex-col gap-4 my-8">
            <button
              onClick={handleStartInterview}
              disabled={isLoading}
              className="w-full p-4 bg-teal-600 rounded-lg text-white text-lg font-bold hover:bg-teal-700 disabled:bg-slate-500"
            >
              é¢æ¥ã‚’é–‹å§‹ã™ã‚‹
            </button>
          </div>
        ) : (
          <div className="flex flex-col gap-4 my-8">
            <h3 className="text-xl font-semibold text-gray-300 mb-3">ã‚ãªãŸã®å›ç­”</h3>

            {isRecording && (
              <div className="w-full text-center p-4 bg-slate-600 rounded-lg text-white">
                <p>éŒ²éŸ³ä¸­ã§ã™...</p>
                <p className="text-sm text-gray-400 mt-2">{currentTranscript}</p>
              </div>
            )}

            {!isFinished && !isRecording ? (
              <button
                onClick={startRecording}
                disabled={isLoading || isTalking}
                className="w-full p-4 bg-teal-600 rounded-lg text-white text-lg font-bold hover:bg-teal-700 disabled:bg-slate-500"
              >
                ğŸ¤ éŸ³å£°ã§å›ç­”ã™ã‚‹
              </button>
            ) : null}

            {!isFinished && isRecording ? (
              <button
                onClick={stopRecording}
                className="w-full p-4 bg-red-600 rounded-lg text-white text-lg font-bold hover:bg-red-700"
              >
                â–  éŒ²éŸ³ã‚’åœæ­¢ã™ã‚‹
              </button>
            ) : null}
          </div>
        )}

        <div />
      </div>
    </main>
  );
}
