'use client';

import { AvatarCanvas } from "@/components/AvatarCanvas";
import { useEffect, useState, useRef } from "react";

// --- å‹å®šç¾© ---
interface ChatMessage {
  role: 'ai' | 'user';
  content: string;
}

interface SpeechRecognition extends EventTarget {
  lang: string;
  interimResults: boolean;
  continuous: boolean;
  onresult: (event: any) => void;
  onend: () => void;
  start: () => void;
  stop: () => void;
}
declare global {
  interface Window {
    SpeechRecognition: { new(): SpeechRecognition };
    webkitSpeechRecognition: { new(): SpeechRecognition };
  }
}
// --- ã“ã“ã¾ã§ ---

export default function Home() {
  const [isTalking, setIsTalking] = useState(false);
  const [isLoading, setIsLoading] = useState(true);
  const [isRecording, setIsRecording] = useState(false);
  const [chatHistory, setChatHistory] = useState<ChatMessage[]>([]);
  const [currentTranscript, setCurrentTranscript] = useState<string>('');
  
  // â˜…â˜…â˜… ä¿®æ­£ç‚¹ 1: æ–°ã—ã„Stateã‚’è¿½åŠ  â˜…â˜…â˜…
  const [interviewStarted, setInterviewStarted] = useState(false);
  
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  // â˜…â˜…â˜… ä¿®æ­£ç‚¹ 2: Audioã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’Refã§ä¿æŒ â˜…â˜…â˜…
  const initialAudioRef = useRef<HTMLAudioElement | null>(null);

  // --- åˆæœŸåŒ–å‡¦ç† ---
  useEffect(() => {
    const prepareInitialQuestion = async () => {
      setIsLoading(true);
      try {
        // éŸ³å£°ã‚’å–å¾—ã™ã‚‹
        const response = await fetch('/api/interview', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ message: "start" }),
        });

        if (!response.ok) throw new Error('API Error');

        const audioBlob = await response.blob();
        const audioUrl = URL.createObjectURL(audioBlob);
        
        // â˜…â˜…â˜… ä¿®æ­£ç‚¹ 3: Audioã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã¦Refã«ä¿å­˜ï¼ˆã¾ã å†ç”Ÿã—ãªã„ï¼‰ â˜…â˜…â˜…
        initialAudioRef.current = new Audio(audioUrl);
        initialAudioRef.current.onended = () => {
          setIsTalking(false);
        };

        // ç”»é¢ã«è¡¨ç¤ºã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚»ãƒƒãƒˆ
        const initialQuestion = "ã“ã‚“ã«ã¡ã¯ï¼AIé¢æ¥ã¸ã‚ˆã†ã“ãã€‚æº–å‚™ãŒã§ããŸã‚‰ä¸‹ã®ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦é–‹å§‹ã—ã¦ãã ã•ã„ã€‚";
        setChatHistory([{ role: 'ai', content: initialQuestion }]);

      } catch (error) {
        console.error("Failed to fetch initial question:", error);
        const errorMessage: ChatMessage = { role: 'ai', content: 'ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚' };
        setChatHistory([errorMessage]);
      } finally {
        setIsLoading(false);
      }
    };

    prepareInitialQuestion();
  }, []);

  // â˜…â˜…â˜… ä¿®æ­£ç‚¹ 4: é¢æ¥ã‚’é–‹å§‹ã™ã‚‹é–¢æ•°ã‚’æ–°è¨­ â˜…â˜…â˜…
  const handleStartInterview = () => {
    if (initialAudioRef.current) {
      setInterviewStarted(true); // é¢æ¥UIã‚’è¡¨ç¤º
      setIsTalking(true);
      initialAudioRef.current.play(); // ã“ã“ã§åˆã‚ã¦å†ç”Ÿï¼
    }
  };

  // --- éŸ³å£°èªè­˜ã‚’é–‹å§‹ã™ã‚‹é–¢æ•° ---
  const startRecording = () => {
    setIsRecording(true);
    setCurrentTranscript('');
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    
    if (!SpeechRecognition) {
      alert("ãŠä½¿ã„ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°èªè­˜ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚");
      setIsRecording(false);
      return;
    }
    
    const recognition = new SpeechRecognition();
    recognition.lang = 'ja-JP';
    recognition.interimResults = true;
    recognition.continuous = false;
    recognitionRef.current = recognition;

    recognition.onresult = (event: any) => {
      let transcript = '';
      for (let i = 0; i < event.results.length; i++) {
        transcript += event.results[i][0].transcript;
      }
      setCurrentTranscript(transcript);
    };

    recognition.onend = () => {
      setIsRecording(false);
      setCurrentTranscript(prev => {
        if (prev.trim()) {
          sendToBackend(prev.trim());
        }
        return '';
      });
    };
    recognition.start();
  };

  // --- ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ã‚’é€ä¿¡ã™ã‚‹é–¢æ•° ---
  const sendToBackend = async (message: string) => {
    const userMessage: ChatMessage = { role: 'user', content: message };
    setChatHistory(prev => [...prev, userMessage]);
    setIsLoading(true);

    try {
      const response = await fetch('/api/interview', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ message: message }),
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`API Error: ${response.status} ${errorText}`);
      }

      const audioBlob = await response.blob();
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);

      setIsTalking(true);
      audio.play();
      audio.onended = () => {
        setIsTalking(false);
      };

      // â˜…â˜…â˜… ä¿®æ­£ç‚¹ 5: æ¬¡ã®è³ªå•ãƒ†ã‚­ã‚¹ãƒˆã‚’APIã‹ã‚‰å—ã‘å–ã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼ˆå°†æ¥çš„ãªæ”¹å–„æ¡ˆï¼‰ â˜…â˜…â˜…
      // ä»Šå›ã¯å›ºå®šãƒ†ã‚­ã‚¹ãƒˆã®ã¾ã¾ã§ã™ãŒã€å°†æ¥çš„ã«ã¯APIã‹ã‚‰è³ªå•ãƒ†ã‚­ã‚¹ãƒˆã‚‚è¿”ã™ã¨ã‚ˆã‚Šè‰¯ããªã‚Šã¾ã™ã€‚
      const aiMessage: ChatMessage = { role: 'ai', content: "ãªã‚‹ã»ã©ã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚æ¬¡ã«..." };
      setChatHistory(prev => [...prev, aiMessage]);
    } catch (error) {
      console.error(error);
      const errorMessage: ChatMessage = { role: 'ai', content: 'ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚' };
      setChatHistory(prev => [...prev, errorMessage]);
    } finally {
      setIsLoading(false);
    }
  };

  const latestAiQuestion = chatHistory.findLast(msg => msg.role === 'ai')?.content;

  return (
    <main className="flex flex-row h-screen bg-gray-900 text-white font-sans">
      <div className="w-2/3 h-full relative">
        <AvatarCanvas isTalking={isTalking} />
      </div>

      <div className="w-1/3 h-full bg-slate-800 p-8 flex flex-col justify-between border-l-2 border-slate-600">
        <div>
          <h2 className="text-2xl font-bold text-teal-300 mb-4 border-b-2 border-teal-500 pb-2">
            AIé¢æ¥
          </h2>
          <div className="bg-slate-700 p-6 rounded-lg shadow-lg min-h-[120px]">
            <p className="text-lg text-gray-200 leading-relaxed">
              {isLoading ? "æº–å‚™ã—ã¦ã„ã¾ã™..." : latestAiQuestion || "..."}
            </p>
          </div>
        </div>

        {/* â˜…â˜…â˜… ä¿®æ­£ç‚¹ 6: UIã®è¡¨ç¤ºã‚’æ¡ä»¶åˆ†å²ã•ã›ã‚‹ â˜…â˜…â˜… */}
        {!interviewStarted ? (
          // é¢æ¥é–‹å§‹å‰ã®è¡¨ç¤º
          <div className="flex flex-col gap-4 my-8">
            <button
              onClick={handleStartInterview}
              disabled={isLoading}
              className="w-full p-4 bg-teal-600 rounded-lg text-white text-lg font-bold hover:bg-teal-700 transition-all duration-200 focus:outline-none focus:ring-2 focus:ring-teal-400 transform hover:scale-105 disabled:bg-slate-500 disabled:cursor-not-allowed"
            >
              é¢æ¥ã‚’é–‹å§‹ã™ã‚‹
            </button>
          </div>
        ) : (
          // é¢æ¥é–‹å§‹å¾Œã®è¡¨ç¤º
          <div className="flex flex-col gap-4 my-8">
            <h3 className="text-xl font-semibold text-gray-300 mb-3">ã‚ãªãŸã®å›ç­”</h3>
            {isRecording && (
              <div className="w-full text-center p-4 bg-slate-600 rounded-lg text-white">
                ãƒã‚¤ã‚¯ã§è©±ã—ã¦ãã ã•ã„...
                <p className="text-sm text-gray-400 mt-2">{currentTranscript}</p>
              </div>
            )}
            
            <button
              onClick={startRecording}
              disabled={isRecording || isLoading || isTalking}
              className="w-full p-4 bg-teal-600 rounded-lg text-white text-lg font-bold hover:bg-teal-700 transition-all duration-200 focus:outline-none focus:ring-2 focus:ring-teal-400 transform hover:scale-105 disabled:bg-slate-500 disabled:cursor-not-allowed"
            >
              ğŸ¤ éŸ³å£°ã§å›ç­”ã™ã‚‹
            </button>
          </div>
        )}
        <div />
      </div>
    </main>
  );
}
